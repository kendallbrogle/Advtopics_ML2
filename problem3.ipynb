{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To copy this template: File -> Save a Copy in Drive\n",
        "\n",
        "***DISCLAIMER**: In case of any discrepancy in the assignment instruction, please refer to the `PDF` document.*"
      ],
      "metadata": {
        "id": "OMAdi9qgC-B9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3 - Perceptron"
      ],
      "metadata": {
        "id": "NcDhlfqyBd6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1"
      ],
      "metadata": {
        "id": "-id00ye6CNLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate training data\n",
        "num_training_points = 10\n",
        "X_train = np.random.rand(num_training_points, 2)\n",
        "y_train = (X_train[:, 0] > X_train[:, 1]).astype(int) * 2 - 1  # Positive class: 1, Negative class: -1\n",
        "\n",
        "# Generate test data\n",
        "num_test_points = 5000\n",
        "X_test = np.random.rand(num_test_points, 2)\n",
        "y_test = (X_test[:, 0] > X_test[:, 1]).astype(int) * 2 - 1\n",
        "\n",
        "# Perceptron training\n",
        "w = np.zeros(X_train.shape[1])\n",
        "num_epochs = 100\n",
        "for _ in range(num_epochs):\n",
        "    for i in range(X_train.shape[0]):\n",
        "        if y_train[i] * np.dot(X_train[i], w) <= 0:\n",
        "            w += y_train[i] * X_train[i]\n",
        "\n",
        "# Perceptron prediction\n",
        "y_pred = np.sign(np.dot(X_test, w))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(f\"Perceptron Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCT58kpzJgMr",
        "outputId": "f60b8752-3e42-4905-fb67-01acff9a42fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perceptron Accuracy: 98.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "The accuracy of the perceptron model on the test data was calculated by comparing the predicted labels to the true labels.\n",
        "The perceptron achieved an accuracy of 98.70%, indicating that it was able to correctly classify the majority of the test data points.\n",
        "This demonstrates the effectiveness of the perceptron algorithm in linearly separable datasets. It successfully learned the decision boundary (the hyperplane defined by x1 - x2 = 0) and made accurate predictions on new, randomly generated data points.\n"
      ],
      "metadata": {
        "id": "4kineJbrfcg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2"
      ],
      "metadata": {
        "id": "e1D_yfneCWqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hinge loss training\n",
        "w_hinge_loss = np.zeros(X_train.shape[1])\n",
        "num_epochs = 100\n",
        "a = 1\n",
        "for _ in range(num_epochs):\n",
        "    for i in range(X_train.shape[0]):\n",
        "        if y_train[i] * np.dot(X_train[i], w_hinge_loss) < a:\n",
        "            w_hinge_loss += y_train[i] * X_train[i]\n",
        "\n",
        "# Hinge loss prediction\n",
        "y_pred_hinge_loss = np.sign(np.dot(X_test, w_hinge_loss))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_hinge_loss = np.mean(y_pred_hinge_loss == y_test)\n",
        "print(f\"Hinge Loss Accuracy: {accuracy_hinge_loss * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWOTWNgqKUrW",
        "outputId": "8ec7a0d2-4284-42c3-b27a-298740c57828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hinge Loss Accuracy: 90.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "The change in the loss function from perceptron criterion to hinge-loss introduced a margin parameter 'a,' which affects the classification decision boundary. This margin allows for a more flexible decision boundary than the traditional perceptron criterion, which aims to separate points as accurately as possible.\n",
        "\n",
        "As a result, the accuracy of the hinge-loss trained model on the same test data points was slightly lower (90.36%) compared to the perceptron model (98.70%). The hinge-loss model allows for some misclassification within the margin, which might result in a lower accuracy but can lead to better generalization and robustness when applied to unseen data."
      ],
      "metadata": {
        "id": "BX8Ns_Imf-BI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3"
      ],
      "metadata": {
        "id": "TpMHZDOECjD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "The perceptron algorithm with the perceptron criterion achieved a better test accuracy (98.7%) compared to the version using hinge loss (90.36%). The reason behind this is that the perceptron criterion aims to classify all training points correctly, which is better for this specific dataset that is linearly separable. It does not introduce a margin and is more focused on minimizing the number of misclassified points.\n",
        "\n",
        "Perceptron Criterion: This loss function aims to correctly classify all training points with no consideration for a margin or soft margin. It looks for 100% accuracy on the training data by adjusting the decision boundary. This often results in overfitting to the training data, as it doesn't allow for any margin or tolerance for misclassification.\n",
        "\n",
        "Hinge-Loss: The hinge-loss introduces a margin of separation, allowing some tolerance for misclassification within the margin. This margin provides a trade-off between training accuracy and the model's ability to generalize to unseen data. While it may lead to slightly lower training accuracy, it often results in better generalization to new data.\n",
        "\n",
        "The perceptron criterion, being more aggressive in minimizing training errors, may result in a higher training accuracy but can lead to poor generalization when applied to unseen data. The hinge-loss introduces a margin that can lead to better test accuracy and more robust model performance."
      ],
      "metadata": {
        "id": "GRo9fm0tf-ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4"
      ],
      "metadata": {
        "id": "YdDLYruzClXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        " The perceptron algorithm is likely to change significantly when using a different set of 10 training points. This is because the perceptron criterion tends to be sensitive to the choice of training points. Different sets of 10 training points may lead to different hyperplanes, resulting in varying classification outcomes.\n",
        "\n",
        "The perceptron algorithm with hinge loss is more stable in terms of classification with different training sets. Hinge loss introduces a margin, and even with different sets of training points, it tends to focus on correctly classifying points that are closer to the decision boundary. As long as the margin is maintained, the classification of the same 5000 test instances is less likely to change significantly with different training point selections"
      ],
      "metadata": {
        "id": "RdaMp6ZBf_DF"
      }
    }
  ]
}